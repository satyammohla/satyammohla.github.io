<!DOCTYPE html>
<html lang="en-US">

<head>
    <link rel="shortcut icon" type="image/x-icon" href="./favicon.ico">
    
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-175602404-1"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }
        gtag('js', new Date());

        gtag('config', 'UA-175602404-1');

    </script>
    <meta name="google-site-verification" content="1jUccmQstmghuo_XVaN01IzZ-UZajRll7-BTn5zvBmU" />


    <!-- Scramble Script by Jeff Donahue -->
    <script src="./data/scramble.js.download"></script>


    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- Begin Jekyll SEO tag v3.8.7 -->
    <title>Satyam Mohla</title>
    <meta name="generator" content="Jekyll v3.8.7">
    <meta property="og:title" content="Satyam Mohla">
    <meta property="og:locale" content="en_US">
    <link rel="canonical" href="https://satyammohla.me">
    <meta property="og:url" content="https://satyammohla.me">
    <meta property="og:site_name" content="Satyam Mohla">
    <script type="application/ld+json">
        {
            "@context": "https://schema.org/",
            "@type": "Person",
            "name": "Satyam Mohla",
            "url": "https://satyammohla.me/",
            "image": "https://satyammohla.me/data/S2_full.jpg",
            "sameAs": [
                "https://twitter.com/SatyamMohla",
                "https://www.instagram.com/msatyammohla",
                "https://www.linkedin.com/in/satyammohla/",
                "https://github.com/satyammohla",
                "https://satyammohla.github.io/"
            ]
        }

    </script>
    <!-- End Jekyll SEO tag -->

    <link rel="stylesheet" href="./data/style.css">
    <!--[if lt IE 9]>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js"></script>
    <![endif]-->

    <!-- UNFURLING -->
    <!-— facebook open graph tags -->
        <meta property="og:type" content="website" />
        <meta property="og:url" content="https://satyammohla.me" />
        <meta property="og:title" content="Satyam Mohla" />
        <meta property="og:description" content="Hi! I am a freshly minted researcher interested in AI Bias, AI for Social Good, Remote Sensing, AI Policy & affordable healthcare." />
        <meta property="og:image" content="https://satyammohla.me/data/S2_full.jpg" />

        <!-— twitter card tags additive with the og: tags -->
            <meta name="twitter:card" content="summary_large_image">
            <meta name="twitter:domain" value="satyammohla.me" />
            <meta name="twitter:title" value="Satyam Mohla" />
            <meta name="twitter:description" value="Hi! I am a freshly minted researcher interested in AI Bias, AI for Social Good, Remote Sensing, AI Policy & affordable healthcare." />
            <meta name="twitter:image" content="https://satyammohla.me/data/S2_full.jpg" />
            <meta name="twitter:url" value="https://satyammohla.me" />
            <meta name="twitter:label1" value="Publications" />
            <meta name="twitter:data1" value="https://satyammohla.me/projects.html" />
            <meta name="twitter:label2" value="Awards & Media" />
            <meta name="twitter:data2" value="https://satyammohla.me/awards.html" />

            <!-- UNFURLING -->


</head>

<body>
    <div class="wrapper">
        <header>
            <h1>
                <center>
                    Satyam Mohla<br>
                    <font size="+2">सत्यम मोहला</font>
                </center>
            </h1>
            <center><img src="./data/S2_small_blue.png" alt="Logo" height="250" width="250" border="0"><br></center>
            <hr style="height:0pt; visibility:hidden;">
            <h3>
                <center>
                    Electrical Engineering<br>
                    IIT Bombay<br>
                </center>
            </h3>

            <p>
                <center>
                    <img style="vertical-align:middle" src="./data/location.png" alt="Location" height="20" width="20">&nbsp;&nbsp;Mumbai, India<br>
                    <img style="vertical-align:middle" src="./data/scholar.png" alt="Location" height="20" width="20"><a href="https://scholar.google.com/citations?hl=en&amp;user=P_AbAbwAAAAJ">&nbsp;&nbsp;Google Scholar</a><br>
<!--
                    <img style="vertical-align:middle" src="./data/pgp.png" alt="PGP Key" height="11" width="26"><a href="./data/PGPkey.txt">&nbsp;&nbsp;258E64EE</a><br> -->
                    <!--0xB14B62AE258E64EE -->
                    <img style="vertical-align:middle" src="./data/email.png" alt="Location" height="11" width="15">
                    <email>
                        <font id="email" style="display:inline;">lamao.@hagmmscytoamil
                            <email>
                                <a href="" onclick="emailScramble.initAnimatedBubbleSort();return false;">&nbsp;unscramble</a> </email>
                        </font>

                        <script>
                            emailScramble = new scrambledString(document.getElementById('email'),
                                'emailScramble', 'ahmaao.l@gmmscytoamil',
                                [10, 8, 6, 1, 4, 19, 17, 16, 11, 12, 20, 5, 0, 18, 3, 2, 7, 14, 13, 15, 9]);

                        </script>
                    </email>
                    <!--
                                        import numpy as np
                                        email='enter email'
                                        scrambled_index=np.random.permutation(len(list(email)))
                                        scrampled_email=''
                                        for i in range(len(list(email))):
                                            scrampled_email+=l[scrambled_index[i]]
                    -->

                </center>
            </p>
        </header>
        <section>

            <div style="text-align: right">
                <font size="+0.5"><a href="./index.html">home</a> • <a href="./research.html">research</a> • <b><a href="./projects.html">projects</a></b> • <a href="./awards.html">awards &amp; media</a></font>
            </div>
            <p></p>
            <h2 id="projects">PROJECTS</h2>
            <!----
-Paper
-Paper
--->

            <hr style="height:5px">

            <h4>Green is the new Black: Multimodal Noisy Segmentation based fragmented burn scars identification in Amazon<br><em>Jan 2020 - Present</em></h4>

            <p>Detection of burn marks due to wildfires in inaccessible rain forests is important for various disaster management and ecological studies. Diverse cropping patterns and the fragmented nature of arable landscapes amidst similar looking land patterns often thwart the precise mapping of burn scars. Recent advances in remote-sensing and availability of multimodal data offer a viable time-sensitive solution to classical methods, which often requires human expert intervention. In this work, we utilised a partially/mis-labelled dataset representing burn patterns in Amazon rainforest to propose AmazonNET - a U-net based segmentation network to correctly identify burn scars & reject incorrect labels, demonstrating our approach as one of the first to effectively utilise deep learning based segmentation models in multimodal burn scar identification.
            </p>

            <p><img style="vertical-align:middle" width="520px" src="./assets/projects/aisg_gif.gif" alt="Saving Amazon Forests Using AI"></p>
            <p>Additional Information: <a href="https://arxiv.org/abs/2009.04634">Webpage</a> / <a href="https://arxiv.org/pdf/2009.04634.pdf">Paper</a> / <a href="https://towardsdatascience.com/green-is-the-new-black-saving-amazon-rainforests-using-ai-ec0676564e9a">Blog</a> / <a href="./assets/projects/google_amazon.bib">bibtex</a></p>
            <!--  <a href="https://towardsdatascience.com/cross-attention-is-what-you-need-fusatnet-fusion-network-b8e6f673491">Blog</a> /  ->
<!----
-Paper
-Paper
--->
            <hr style="height:5px">

            <h4>Cross-Attention is what you need: Multimodal Fusion Network for robust Classification in Remote sensing<br>
                <em>May 2019 - Present</em></h4>
            <p>With recent advances in sensing, multimodal data is becoming easily available for various applications, especially in remote sensing (RS), where many data types like multispectral (MSI), hyperspectral (HSI), LiDAR etc. are available. Effective fusion of these multisource datasets is becoming important, for these multimodality features have been shown to generate highly accurate land-cover maps. However, fusion in the context of RS is non-trivial considering the redundancy involved in the data and the large domain differences among multiple modalities. In addition, the feature extraction modules for different modalities hardly interact among themselves, which further limits their semantic relatedness. As a remedy, we propose a feature fusion and extraction framework, namely FusAtNet, for collective land-cover classification of HSIs and LiDAR data in this paper. The proposed framework effectively utilizses HSI modality to generate an attention map using "self-attention" mechanism that highlights its own spectral features. Similarly, a "cross-attention" approach is simultaneously used to harness the LiDAR derived attention map that accentuates the spatial features of HSI. These attentive spectral and spatial representations are then explored further along with the original data to obtain modality-specific feature embeddings. The modality oriented joint spectro-spatial information thus obtained, is subsequently utilized to carry out the land-cover classification task. Experimental evaluations on three HSI-LiDAR datasets show that the proposed method achieves the state-of-the-art classification performance, including on the largest HSI-LiDAR dataset available, Houston, opening new avenues in multimodality feature fusion classification.</p>

            <p><img style="vertical-align:middle" width="520px" src="./assets/projects/pbvs_gif100.gif" alt="MUUFL Data"></p>

            <p>Additional Information: <a href="https://openaccess.thecvf.com/content_CVPRW_2020/html/w6/Mohla_FusAtNet_Dual_Attention_Based_SpectroSpatial_Multimodal_Fusion_Network_for_Hyperspectral_CVPRW_2020_paper.html">Webpage</a> / <a href="https://openaccess.thecvf.com/content_CVPRW_2020/papers/w6/Mohla_FusAtNet_Dual_Attention_Based_SpectroSpatial_Multimodal_Fusion_Network_for_Hyperspectral_CVPRW_2020_paper.pdf">Paper</a> / <a href="https://towardsdatascience.com/cross-attention-is-what-you-need-fusatnet-fusion-network-b8e6f673491">Blog</a> / <a href="./assets/projects/google_PBVS.bib">bibtex</a></p>



            <!----
-Paper
-Paper
--->
            <hr style="height:5px">

            <h4 id="ICLR">Mimicking Human Cognitive Models to resolve Texture-Shape Bias<br>
                <em>May 2019 - Present</em></h4>
            <p>Recent works demonstrate the texture bias in Convolutional Neural Networks (CNNs), conflicting with early works claiming that networks identify objects using shape. It is commonly believed that the cost function forces the network to take a greedy route to increase accuracy using texture, failing to explore any global statistics. We propose a novel intuitive architecture, namely CognitiveCNN, inspired from feature integration theory in psychology to utilise human-interpretable feature like shape, texture, edges etc. to reconstruct, and classify the image. We define two metrics, namely TIC and RIC to quantify the importance of each stream using attention maps. We introduce a regulariser which ensures that the contribution of each feature is same for any task, as it is for reconstruction; and perform experiments to show the resulting boost in accuracy and robustness besides imparting explainability. Lastly, we adapt these ideas to conventional CNNs and propose Augmented Cognitive CNN to achieve superior performance in recognition. <br></p>

            <p><img style="vertical-align:middle" width="520px" src="./assets/projects/iclr.jpg" alt="Unbiased Training Pipeline"></p>

            <p>Additional Information: <a href="https://baicsworkshop.github.io/program/baics_27.html">Webpage</a> / <a href="https://arxiv.org/abs/2006.14722">Paper</a> / <a href="./assets/projects/google_ICLR.bib">bibtex</a></p>

            <!----
-Paper
-Paper
--->
            <hr style="height:5px">

            <h4>Design and Development of Point of Care Test and Optical Reader for Early Screening of Kidney Related Disorder
                <br><em>Dec 2018-Present</em></h4>
            <p>Low-cost, paper-based colorimetric assays for early screening of albumin, creatinine and their ratio are developed. The developed methods are non-invasive and require only 10 μl of the urine sample. A reflectance-based optical reader is also developed for the quantification of the albumin and creatinine. The developed method is based on spot-urine testing, which is advantageous when compared to the conventional 24-hr urine collection. The detection range of albumin and creatinine assays is 10–150 mg/dL and 25–400 mg/dL, respectively. The developed assays and optical reader were tested with the chronic kidney diseased patient's samples at KEM Hospital, Mumbai.<br>

                <p><img style="vertical-align:middle" width="520px" src="./assets/projects/nanomed.png" alt="Detecting Kidney disorders"></p>
                <p>Additional Information: <a href="https://ieeexplore.ieee.org/document/9130619">Webpage</a> / <a href="https://www.biorxiv.org/content/10.1101/2020.07.01.181024v1.full.pdf">Paper</a> / <a href="./assets/projects/google_nanomed.bib">bibtex</a></p>
                <!----
-Paper
-Paper
--->
                <hr style="height:5px">

                <h4>Design and Development of Quantum Dots Infused Films and an Optical Reader for Measurement of Blood Electrolytes
                    <br><em>Dec 2018-Present</em></h4>
                <p>pH-sensitive, quantum dots (QDs) based sensor films were developed for point-of-care diagnosis and monitoring of plasma sodium and potassium concentrations. MPA capped CdTe QDs are synthesized using the hydrothermal process. The emission wavelength of the synthesized QDs is programmed at 635 nm. Characterization of QDs was done using transmission electron microscopy 200KV (TEM), X-ray diffraction (XRD), fluorescence spectroscopy, which indicated the excellent photostability of the developed QDs. Chromoionophore-I, II, Potassium ionophore, Sodium ionophore, cation exchanger, QDs, PVC and tetrahydrofuran (THF) were used to developed sodium and potassium sensor films. The sensor films exhibit good fluorescence in the presence of plasma sodium or potassium ions. The fluorescence intensity is directly proportional to the concentration of sodium or potassium ions present in the plasma. An optical reader is also developed for the quantification of the fluorescence intensity of the sodium and potassium sensor films.<br>

                    <p><img style="vertical-align:middle" width="247px" src="./assets/projects/nmdc.gif" alt="Detecting blood electrolytic imbalance"><img style="vertical-align:middle" width="270px" src="./assets/projects/techfest.jpg" alt="Techfest"></p>

                    <p>Additional Information: <a href="https://ieeexplore.ieee.org/abstract/document/9084002">Paper</a> / <a href="./assets/projects/google_nmdc.bib">bibtex</a></p>
                    <!----
-Paper
-Paper
--->
                    <hr style="height:5px">

                    <h4>IIT Bombay Mars Rover Team: Division Head, Research and Biosciences
                        <br><em>Sept 2016 - Aug 2018 </em></h4>
                    <p>The IITB Mars Rover project is a student initiative at IIT Bombay to build a prototype Mars
                        rover capable of performing extra-terrestrial robotics and carrying out scientific experiments, participating in the <a href="http://urc.marssociety.org/home">University Rover Challenge</a> at the Mars Society’s Mars Desert Research Station, Utah. I served as the division head of <em>Biosciences &amp; Research</em>, responsible for the instumentation and biosensing stack on the rover and performing autonomous sample collection and onboard scientific experimentation. I often coordinate with Electical Division for ROS integration, and with Mechanical division to work on sample collection. Biosciences consists of Bioassembly, Astrobiology, Geosciences and Research sub-divisions. As such I am also responsible for developing and acquiring novel research tools, like adapting an pocket-size NIR micro-spectrophotometer called <a href="https://www.consumerphysics.com/">Scio</a> to our onboard rover sensing stack.</p>

                    <p><img style="vertical-align:middle" width="520px" src="./assets/projects/utah.jpg" alt="University Rover Challenge"></p>
                    <p>Additional Information: <a href="https://www.youtube.com/watch?v=4l9flVxJVNk">2018 Promo Video</a> / <a href="https://www.youtube.com/watch?v=PeovDDTiEGU">2017 Promo Video</a> / <a href="./assets/projects/MSI_interIIT_2018.pdf">Poster</a></p>
                    <!----
-Paper
-Paper
--->
                    <hr style="height:5px">

                    <h4>Co-Founder, TechForSociety, Social Initiative
                        <br><em>Nov 2017 - Present</em></h4>
                    <p>TechForSociety is a social project started with an aim to utilize the young engineers of tomorrow in premier technical institutes like IITs to cater to the technology needs of rural areas. We aim to develop grassroots understanding to work on fundamental social issues and design low cost technology interventions and solutions to solve various problems faced by society today. We have been active for more than two years and have developed many prototypes for water accessibility & water management problems in villages. We have advised various NGOs at grassroots level on technical issues & already created huge impact in those areas, and won various grants and awards.<br>

                        <p><img style="vertical-align:middle" width="520px" src="./assets/projects/techforsociety.gif" alt="TechForSociety Water Initiative"></p>

                        <hr style="height:5px">



        </section>
    </div>
    <script src="./data/scale.fix.js.download"></script>



</body>



<!--Public Key
-----BEGIN PGP PUBLIC KEY BLOCK-----
Version: SKS 1.1.6
Comment: Hostname: pgp.mit.edu
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=ZpwR
-----END PGP PUBLIC KEY BLOCK-----

-->

</html>
