<!DOCTYPE html>
<html lang="en-US">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-175602404-1"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }
        gtag('js', new Date());

        gtag('config', 'UA-175602404-1');

    </script>
    <meta name="google-site-verification" content="1jUccmQstmghuo_XVaN01IzZ-UZajRll7-BTn5zvBmU" />


    <!-- Scramble Script by Jeff Donahue -->
    <script src="./data/scramble.js.download"></script>

    
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- Begin Jekyll SEO tag v3.8.7 -->
    <title>Satyam Mohla</title>
    <meta name="generator" content="Jekyll v3.8.7">
    <meta property="og:title" content="Satyam Mohla">
    <meta property="og:locale" content="en_US">
    <link rel="canonical" href="https://satyammohla.github.io/projects.html">
    <meta property="og:url" content="https://satyammohla.github.io/projects.html">
    <meta property="og:site_name" content="Satyam Mohla">
    <script type="application/ld+json">
        {
            "@type": "WebPage",
            "headline": "Satyam Mohla",
            "url": "https://satyammohla.github.io/projects.html",
            "publisher": {
                "@type": "Organization",
                "logo": {
                    "@type": "ImageObject",
                    "url": "https://satyammohla.github.io/S2_small_blue.png"
                }
            },
            "@context": "https://schema.org"
        }

    </script>
    <!-- End Jekyll SEO tag -->

    <link rel="stylesheet" href="./data/style.css">
    <!--[if lt IE 9]>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js"></script>
    <![endif]-->

<!-- UNFURLING -->
    <!-— facebook open graph tags -->
        <meta property="og:type" content="website" />
        <meta property="og:url" content="https://satyammohla.me" />
        <meta property="og:title" content="Satyam Mohla" />
        <meta property="og:description" content="Hi! I am a freshly minted researcher interested in AI Bias, AI for Social Good, Remote Sensing, AI Policy & affordable healthcare." />
        <meta property="og:image" content="https://satyammohla.me/data/S2_full.jpg" />

    <!-— twitter card tags additive with the og: tags -->
        <meta name="twitter:card" content="summary_large_image">
        <meta name="twitter:domain" value="satyammohla.me" />
        <meta name="twitter:title" value="Satyam Mohla" />
        <meta name="twitter:description" value="Hi! I am a freshly minted researcher interested in AI Bias, AI for Social Good, Remote Sensing, AI Policy & affordable healthcare." />
        <meta name="twitter:image" content="https://satyammohla.me/data/S2_full.jpg" />
        <meta name="twitter:url" value="https://satyammohla.me" />
        <meta name="twitter:label1" value="Publications" />
        <meta name="twitter:data1" value="https://satyammohla.me/projects.html" />
        <meta name="twitter:label2" value="Awards & Media" />
        <meta name="twitter:data2" value="https://satyammohla.me/awards.html" />

<!-- UNFURLING -->


</head>

<body>
    <div class="wrapper">
        <header>
            <h1>
                <center>
                Satyam Mohla<br>
                  <font size="+2">सत्यम मोहला</font>
                </center>
            </h1>
            <center><img src="./data/S2_small_blue.png" alt="Logo" height="250" width="250" border="0"><br></center>
            <hr style="height:0pt; visibility:hidden;">
            <h3>
                <center>
                    Electrical Engineering<br>
                    IIT Bombay<br>
                </center>
            </h3>

            <p>
                <center>
                    <img style="vertical-align:middle" src="./data/location.png" alt="Location" height="20" width="20">&nbsp;&nbsp;Mumbai, India<br>
                    <img style="vertical-align:middle" src="./data/scholar.png" alt="Location" height="20" width="20"><a href="https://scholar.google.com/citations?hl=en&amp;user=P_AbAbwAAAAJ">&nbsp;&nbsp;Google Scholar</a><br>
                    <img style="vertical-align:middle" src="./data/email.png" alt="Location" height="11" width="15">&nbsp;&nbsp;satyammohla@gmail.com<br>                    
                </center>
            </p>
        </header>
        <section>

            <div style="text-align: right">
                <font size="+0.5"><a href="./index.html">home</a> • <a href="./research.html">research</a> • <b><a href="./projects.html">projects</a></b> • <a href="./awards.html">awards &amp; media</a></font>
            </div>
            <p></p>
            <h2 id="projects">PROJECTS</h2>
<!----
-Paper
-Paper
--->            
            <hr style="height:5px">

            <h4>Green is the new Black: Multimodal Noisy Segmentation based fragmented burn scars identification in Amazon<br><em>Jan 2020 - Present</em></h4>
            
            <p>Detection of burn marks due to wildfires in inaccessible rain forests is important for various disaster management and ecological studies. Diverse cropping patterns and the fragmented nature of arable landscapes amidst similar looking land patterns often thwart the precise mapping of burn scars. Recent advances in remote-sensing and availability of multimodal data offer a viable time-sensitive solution to classical methods, which often requires human expert intervention. In this work, we utilised a partially/mis-labelled dataset representing burn patterns in Amazon rainforest to propose AmazonNET - a U-net based segmentation network to correctly identify burn scars & reject incorrect labels, demonstrating our approach as one of the first to effectively utilise deep learning based segmentation models in multimodal burn scar identification.
            </p>

            <p><img style="vertical-align:middle" width="520px" src="./assets/projects/aisg_gif.gif" alt="Saving Amazon Forests Using AI"></p>
            <p>Additional Information: <a href="https://crcs.seas.harvard.edu/publications/green-new-black-multimodal-noisy-segmentation-based-fragmented-burn-scars">Webpage</a> / <a href="https://crcs.seas.harvard.edu/files/crcs/files/ai4sg_2020_paper_11.pdf">Paper</a> / <a href="./assets/projects/google_amazon.bib">bibtex</a></p>
<!--  <a href="https://towardsdatascience.com/cross-attention-is-what-you-need-fusatnet-fusion-network-b8e6f673491">Blog</a> /  ->
<!----
-Paper
-Paper
--->
            <hr style="height:5px">

            <h4>Cross-Attention is what you need: Multimodal Fusion Network for robust Classification in Remote sensing<br>
                <em>May 2019 - Present</em></h4>
            <p>With recent advances in sensing, multimodal data is becoming easily available for various applications, especially in remote sensing (RS), where many data types like multispectral (MSI), hyperspectral (HSI), LiDAR etc. are available. Effective fusion of these multisource datasets is becoming important, for these multimodality features have been shown to generate highly accurate land-cover maps. However, fusion in the context of RS is non-trivial considering the redundancy involved in the data and the large domain differences among multiple modalities. In addition, the feature extraction modules for different modalities hardly interact among themselves, which further limits their semantic relatedness. As a remedy, we propose a feature fusion and extraction framework, namely FusAtNet, for collective land-cover classification of HSIs and LiDAR data in this paper. The proposed framework effectively utilizses HSI modality to generate an attention map using "self-attention" mechanism that highlights its own spectral features. Similarly, a "cross-attention" approach is simultaneously used to harness the LiDAR derived attention map that accentuates the spatial features of HSI. These attentive spectral and spatial representations are then explored further along with the original data to obtain modality-specific feature embeddings. The modality oriented joint spectro-spatial information thus obtained, is subsequently utilized to carry out the land-cover classification task. Experimental evaluations on three HSI-LiDAR datasets show that the proposed method achieves the state-of-the-art classification performance, including on the largest HSI-LiDAR dataset available, Houston, opening new avenues in multimodality feature fusion classification.</p>

            <p><img style="vertical-align:middle" width="520px" src="./assets/projects/pbvs_gif100.gif" alt="MUUFL Data"></p>

            <p>Additional Information: <a href="https://openaccess.thecvf.com/content_CVPRW_2020/html/w6/Mohla_FusAtNet_Dual_Attention_Based_SpectroSpatial_Multimodal_Fusion_Network_for_Hyperspectral_CVPRW_2020_paper.html">Webpage</a> / <a href="https://openaccess.thecvf.com/content_CVPRW_2020/papers/w6/Mohla_FusAtNet_Dual_Attention_Based_SpectroSpatial_Multimodal_Fusion_Network_for_Hyperspectral_CVPRW_2020_paper.pdf">Paper</a> / <a href="https://towardsdatascience.com/cross-attention-is-what-you-need-fusatnet-fusion-network-b8e6f673491">Blog</a> / <a href="./assets/projects/google_PBVS.bib">bibtex</a></p>


            
<!----
-Paper
-Paper
--->
            <hr style="height:5px">

            <h4 id="ICLR">Mimicking Human Cognitive Models to resolve Texture-Shape Bias<br>
                <em>May 2019 - Present</em></h4>
            <p>Recent works demonstrate the texture bias in Convolutional Neural Networks (CNNs), conflicting with early works claiming that networks identify objects using shape. It is commonly believed that the cost function forces the network to take a greedy route to increase accuracy using texture, failing to explore any global statistics. We propose a novel intuitive architecture, namely CognitiveCNN, inspired from feature integration theory in psychology to utilise human-interpretable feature like shape, texture, edges etc. to reconstruct, and classify the image. We define two metrics, namely TIC and RIC to quantify the importance of each stream using attention maps. We introduce a regulariser which ensures that the contribution of each feature is same for any task, as it is for reconstruction; and perform experiments to show the resulting boost in accuracy and robustness besides imparting explainability. Lastly, we adapt these ideas to conventional CNNs and propose Augmented Cognitive CNN to achieve superior performance in recognition. <br></p>

            <p><img style="vertical-align:middle" width="520px" src="./assets/projects/iclr.jpg" alt="Unbiased Training Pipeline"></p>

            <p>Additional Information: <a href="https://baicsworkshop.github.io/program/baics_27.html">Webpage</a> / <a href="https://arxiv.org/abs/2006.14722">Paper</a> / <a href="./assets/projects/google_ICLR.bib">bibtex</a></p>

<!----
-Paper
-Paper
--->            
            <hr style="height:5px">

            <h4>Design and Development of Point of Care Test and Optical Reader for Early Screening of Kidney Related Disorder
            <br><em>Dec 2018-Present</em></h4>
            <p>Low-cost, paper-based colorimetric assays for early screening of albumin, creatinine and their ratio are developed. The developed methods are non-invasive and require only 10 μl of the urine sample. A reflectance-based optical reader is also developed for the quantification of the albumin and creatinine. The developed method is based on spot-urine testing, which is advantageous when compared to the conventional 24-hr urine collection. The detection range of albumin and creatinine assays is 10–150 mg/dL and 25–400 mg/dL, respectively. The developed assays and optical reader were tested with the chronic kidney diseased patient's samples at KEM Hospital, Mumbai.<br>

            <p><img style="vertical-align:middle" width="520px" src="./assets/projects/nanomed.png" alt="Detecting Kidney disorders"></p>
            <p>Additional Information: <a href="https://ieeexplore.ieee.org/document/9130619">Webpage</a> / <a href="https://www.biorxiv.org/content/10.1101/2020.07.01.181024v1.full.pdf">Paper</a> / <a href="./assets/projects/google_nanomed.bib">bibtex</a></p>
<!----
-Paper
-Paper
--->            
            <hr style="height:5px">

            <h4>Design and Development of Quantum Dots Infused Films and an Optical Reader for Measurement of Blood Electrolytes
            <br><em>Dec 2018-Present</em></h4>
            <p>pH-sensitive, quantum dots (QDs) based sensor films were developed for point-of-care diagnosis and monitoring of plasma sodium and potassium concentrations. MPA capped CdTe QDs are synthesized using the hydrothermal process. The emission wavelength of the synthesized QDs is programmed at 635 nm. Characterization of QDs was done using transmission electron microscopy 200KV (TEM), X-ray diffraction (XRD), fluorescence spectroscopy, which indicated the excellent photostability of the developed QDs. Chromoionophore-I, II, Potassium ionophore, Sodium ionophore, cation exchanger, QDs, PVC and tetrahydrofuran (THF) were used to developed sodium and potassium sensor films. The sensor films exhibit good fluorescence in the presence of plasma sodium or potassium ions. The fluorescence intensity is directly proportional to the concentration of sodium or potassium ions present in the plasma. An optical reader is also developed for the quantification of the fluorescence intensity of the sodium and potassium sensor films.<br>

            <p><img style="vertical-align:middle" width="247px" src="./assets/projects/nmdc.gif" alt="Detecting blood electrolytic imbalance"><img style="vertical-align:middle" width="270px" src="./assets/projects/techfest.jpg" alt="Techfest"></p>
            
            <p>Additional Information: <a href="https://ieeexplore.ieee.org/abstract/document/9084002">Paper</a> / <a href="./assets/projects/google_nmdc.bib">bibtex</a></p>
<!----
-Paper
-Paper
--->            
            <hr style="height:5px">

            <h4>IIT Bombay Mars Rover Team: Division Head, Research and Bisensors
            <br><em>Sept 2016 - Aug 2018 </em></h4>
            <p>The IITB Mars Rover project is a student initiative at IIT Bombay to build a prototype Mars
            rover capable of performing extra-terrestrial robotics and carrying out scientific experiments, participating in the <a href="http://urc.marssociety.org/home">University Rover Challenge</a> at the Mars Society’s Mars Desert Research Station, Utah. I serve as the division head of <em>Biosciences &amp; Research</em>, responsible for the instumentation and biosensing stack on the rover and performing autonomous sample collection and onboard scientific experimentation. I also coordinate with Electical Division and am responsible for ROS integration, and with Mechanical division to work on sample collection. Biosciences consists of Bioassembly, Astrobiology, Geosciences and Research sub-divisions. As such I am also responsible for developing and acquiring novel research tools, like a rover-adapted onboard pocket-size NIR micro-spectrophotometer called <a href="https://www.consumerphysics.com/">Scio</a>.</p>

            <p><img style="vertical-align:middle" width="520px" src="./assets/projects/utah.jpg" alt="University Rover Challenge"></p>
            <p>Additional Information: <a href="https://www.youtube.com/watch?v=4l9flVxJVNk">2018 SAR Promo Video</a> / <a href="https://www.youtube.com/watch?v=PeovDDTiEGU">2017 SAR Promo Video</a> / <a href="./assets/projects/MSI_interIIT_2018.pdf">Poster</a></p>
<!----
-Paper
-Paper
--->            
            <hr style="height:5px">

            <h4>Co-Founder, TechForSociety, Social Initiative
            <br><em>Nov 2017 - Present</em></h4>
            <p>TechForSociety is a social project started with an aim to utilize the young engineers of tomorrow in premier technical institutes like IITs to cater to the technology needs of rural areas. We aim to develop grassroots understanding to work on fundamental social issues and design low cost technology interventions and solutions to solve various problems faced by society today. We have been active for more than two years and have developed many prototypes for water accessibility & water management problems in villages. We have advised various NGOs at grassroots level on technical issues & already created huge impact in those areas, and won various grants and awards.<br>

            <p><img style="vertical-align:middle" width="520px" src="./assets/projects/techforsociety.gif" alt="TechForSociety Water Initiative"></p>
            
            <hr style="height:5px">



        </section>
    </div>
    <script src="./data/scale.fix.js.download"></script>



</body>

</html>
